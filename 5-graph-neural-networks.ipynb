{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XuXWJLEm2UWS"
      },
      "source": [
        "# CS245 - Lab 5\n",
        "## Graph Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gzsP50bF6Gb"
      },
      "source": [
        "In Lab 5, we will work to construct our own graph neural network using PyTorch Geometric (PyG) and then apply that model on two Open Graph Benchmark (OGB) datasets. These two datasets will be used to benchmark your model's performance on node property prediction, predicting properties of single nodes.\n",
        "\n",
        "First, we will learn how PyTorch Geometric stores graphs as PyTorch tensors.\n",
        "\n",
        "Then, we will load and inspect one of the Open Graph Benchmark (OGB) datasets by using the `ogb` package. OGB is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. The `ogb` package not only provides data loaders for each dataset but also model evaluators.\n",
        "\n",
        "Lastly, we will build our own graph neural network using PyTorch Geometric. We will then train and evaluate our model on the OGB node property prediction tasks.\n",
        "\n",
        "**Note**: Make sure to **sequentially run all the cells in each section**, so that the intermediate variables / packages will carry over to the next cell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGKqVEbbMEzf"
      },
      "source": [
        "# Device\n",
        "You might need to use a GPU for this Colab to run quickly.\n",
        "\n",
        "Please click `Accelerator` on the right side pannel and select **GPU**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCK7krJdp4o8"
      },
      "source": [
        "# Setup\n",
        "The installation of PyG on Kaggle can be a little bit tricky. First let us check which version of PyTorch you are running"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2vkP8pA1qBE5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PyTorch has version 1.11.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(\"PyTorch has version {}\".format(torch.__version__))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6d22O6DqGSZ"
      },
      "source": [
        "Download the necessary packages for PyG. Make sure that your version of torch matches the output from the cell above. In case of any issues, more information can be found on the [PyG's installation page](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "zr8hfxJ-qRg2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-scatter\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_scatter-2.0.9-cp37-cp37m-linux_x86_64.whl (7.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.9 MB 4.3 MB/s \n",
            "\u001b[?25hInstalling collected packages: torch-scatter\n",
            "Successfully installed torch-scatter-2.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Looking in links: https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
            "Collecting torch-sparse\n",
            "  Downloading https://data.pyg.org/whl/torch-1.11.0%2Bcu113/torch_sparse-0.6.13-cp37-cp37m-linux_x86_64.whl (3.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.5 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-sparse) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scipy->torch-sparse) (1.21.6)\n",
            "Installing collected packages: torch-sparse\n",
            "Successfully installed torch-sparse-0.6.13\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torch-geometric\n",
            "  Downloading torch_geometric-2.0.4.tar.gz (407 kB)\n",
            "\u001b[K     |████████████████████████████████| 407 kB 15.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (4.64.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (2.23.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (3.0.9)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from torch-geometric) (1.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->torch-geometric) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->torch-geometric) (2022.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->torch-geometric) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torch-geometric) (2022.5.18.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->torch-geometric) (1.1.0)\n",
            "Building wheels for collected packages: torch-geometric\n",
            "  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch-geometric: filename=torch_geometric-2.0.4-py3-none-any.whl size=616603 sha256=03728fde74f84d83f59123c900e89f4156b9a5e0e1b436aac8cea1f6bd110355\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/a6/a4/ca18c3051fcead866fe7b85700ee2240d883562a1bc70ce421\n",
            "Successfully built torch-geometric\n",
            "Installing collected packages: torch-geometric\n",
            "Successfully installed torch-geometric-2.0.4\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.3-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 6.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.21.6)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.3.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.11.0+cu113)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.64.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.2)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2022.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (4.2.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2022.5.18.1)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=fa5bc3ad95e801f263ae97f5949c96893844bd10fb7f5da6df48fe9e404af653\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.3 outdated-0.2.1\n"
          ]
        }
      ],
      "source": [
        "# Install torch geometric\n",
        "!pip install torch-scatter -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-sparse -f https://pytorch-geometric.com/whl/torch-1.11.0+cu113.html\n",
        "!pip install torch-geometric\n",
        "!pip install ogb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXa7yIG4E0Fp"
      },
      "source": [
        "# Open Graph Benchmark (OGB)\n",
        "\n",
        "The Open Graph Benchmark (OGB) is a collection of realistic, large-scale, and diverse benchmark datasets for machine learning on graphs. Its datasets are automatically downloaded, processed, and split using the OGB Data Loader. The model performance can then be evaluated by using the OGB Evaluator in a unified manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnazPGGAJAZN"
      },
      "source": [
        "## Dataset and Data\n",
        "\n",
        "OGB also supports PyG dataset and data classes. Here we take a look on the `ogbn-arxiv` dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Gpc6bTm3GF02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/arxiv.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.08 GB: 100%|██████████| 81/81 [00:07<00:00, 10.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting dataset/arxiv.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing...\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 9383.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Converting graphs into PyG objects...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 6442.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "Done!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The ogbn-arxiv dataset has 1 graph\n",
            "Data(num_nodes=169343, x=[169343, 128], node_year=[169343, 1], y=[169343, 1], adj_t=[169343, 169343, nnz=1166243])\n"
          ]
        }
      ],
      "source": [
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset\n",
        "\n",
        "dataset_name = 'ogbn-arxiv'\n",
        "# Load the dataset and transform it to sparse tensor\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "print('The {} dataset has {} graph'.format(dataset_name, len(dataset)))\n",
        "\n",
        "# Extract the graph\n",
        "data = dataset[0]\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cw0xZJKZI-n3"
      },
      "source": [
        "### Question: How many features are in the ogbn-arxiv graph?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZP844_nT2ZJl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The graph has 128 features\n"
          ]
        }
      ],
      "source": [
        "def graph_num_features(data):\n",
        "    # TODO: Implement a function that takes a PyG data object,\n",
        "    # and returns the number of features in the graph (as an integer).\n",
        "\n",
        "    num_features = 0\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~1 line of code)\n",
        "    num_features = data.num_features\n",
        "    #########################################\n",
        "\n",
        "    return num_features\n",
        "\n",
        "num_features = graph_num_features(data)\n",
        "print('The graph has {} features'.format(num_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DP_yEQZ0NVW"
      },
      "source": [
        "# GNN: Node Property Prediction\n",
        "\n",
        "In this section we will build our first graph neural network using PyTorch Geometric. Then we will apply it to the task of node property prediction (node classification).\n",
        "\n",
        "Specifically, we will use GCN as the foundation for your graph neural network ([Kipf et al. (2017)](https://arxiv.org/pdf/1609.02907.pdf)). To do so, we will work with PyG's built-in `GCNConv` layer. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4CcOUEoInjD"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-DCtgcHpGIpd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.11.0+cu113\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "print(torch.__version__)\n",
        "\n",
        "# The PyG built-in GCNConv\n",
        "from torch_geometric.nn import GCNConv\n",
        "\n",
        "import torch_geometric.transforms as T\n",
        "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IK9z0wQIwzQ"
      },
      "source": [
        "## Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "0ibJ0ieoIwQM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "dataset_name = 'ogbn-arxiv'\n",
        "dataset = PygNodePropPredDataset(name=dataset_name,\n",
        "                                transform=T.ToSparseTensor())\n",
        "data = dataset[0]\n",
        "\n",
        "# Make the adjacency matrix to symmetric\n",
        "data.adj_t = data.adj_t.to_symmetric()\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# If you use GPU, the device should be cuda\n",
        "print('Device: {}'.format(device))\n",
        "\n",
        "data = data.to(device)\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx = split_idx['train'].to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OgUA815bNJ8w"
      },
      "source": [
        "## GCN Model\n",
        "\n",
        "Now we will implement our GCN model!\n",
        "\n",
        "Please follow the figure below to implement the `forward` function.\n",
        "\n",
        "\n",
        "![test](https://drive.google.com/uc?id=128AuYAXNXGg7PIhJJ7e420DoPWKb-RtL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IgspXTYpNJLA"
      },
      "outputs": [],
      "source": [
        "class GCN(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers,\n",
        "                 dropout, return_embeds=False):\n",
        "        # TODO: Implement a function that initializes self.convs, \n",
        "        # self.bns, and self.softmax.\n",
        "\n",
        "        super(GCN, self).__init__()\n",
        "\n",
        "        # A list of GCNConv layers\n",
        "        self.convs = None\n",
        "\n",
        "        # A list of 1D batch normalization layers\n",
        "        self.bns = None\n",
        "\n",
        "        # The log softmax layer\n",
        "        self.softmax = None\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. You should use torch.nn.ModuleList for self.convs and self.bns\n",
        "        ## 2. self.convs has num_layers GCNConv layers\n",
        "        ## 3. self.bns has num_layers - 1 BatchNorm1d layers\n",
        "        ## 4. You should use torch.nn.LogSoftmax for self.softmax\n",
        "        ## 5. The parameters you can set for GCNConv include 'in_channels' and \n",
        "        ## 'out_channels'. For more information please refer to the documentation:\n",
        "        ## https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GCNConv\n",
        "        ## 6. The only parameter you need to set for BatchNorm1d is 'num_features'\n",
        "        ## For more information please refer to the documentation: \n",
        "        ## https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html\n",
        "        ## (~10 lines of code)\n",
        "        self.convs = torch.nn.ModuleList(\n",
        "            [GCNConv(input_dim, hidden_dim)] + \\\n",
        "            [GCNConv(hidden_dim, hidden_dim) for _ in range(num_layers - 2)] + \\\n",
        "            [GCNConv(hidden_dim, output_dim)]\n",
        "        )\n",
        "\n",
        "        self.bns = torch.nn.ModuleList(\n",
        "            [torch.nn.BatchNorm1d(hidden_dim) for _ in range(num_layers - 1)]\n",
        "        )\n",
        "        \n",
        "        self.softmax = torch.nn.LogSoftmax()\n",
        "        #########################################\n",
        "\n",
        "        # Probability of an element getting zeroed\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # Skip classification layer and return node embeddings\n",
        "        self.return_embeds = return_embeds\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        for conv in self.convs:\n",
        "            conv.reset_parameters()\n",
        "        for bn in self.bns:\n",
        "            bn.reset_parameters()\n",
        "\n",
        "    def forward(self, x, adj_t):\n",
        "        # TODO: Implement a function that takes the feature tensor x and\n",
        "        # edge_index tensor adj_t and returns the output tensor as\n",
        "        # shown in the figure.\n",
        "\n",
        "        out = None\n",
        "\n",
        "        ############# Your code here ############\n",
        "        ## Note:\n",
        "        ## 1. Construct the network as shown in the figure\n",
        "        ## 2. torch.nn.functional.relu and torch.nn.functional.dropout are useful\n",
        "        ## For more information please refer to the documentation:\n",
        "        ## https://pytorch.org/docs/stable/nn.functional.html\n",
        "        ## 3. Don't forget to set F.dropout training to self.training\n",
        "        ## 4. If return_embeds is True, then skip the last softmax layer\n",
        "        ## (~7 lines of code)\n",
        "        for i, conv in enumerate(self.convs):\n",
        "            x = conv(x, adj_t)\n",
        "            if i < len(self.bns):\n",
        "                x = self.bns[i](x)\n",
        "                x = F.relu(x)\n",
        "                x = F.dropout(x, self.dropout, training=self.training)\n",
        "        if self.return_embeds:\n",
        "            out = x\n",
        "        else:\n",
        "            out = self.softmax(x)\n",
        "        #########################################\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FF1hnHUhO81e"
      },
      "outputs": [],
      "source": [
        "def train(model, data, train_idx, optimizer, loss_fn):\n",
        "    # TODO: Implement a function that trains the model by \n",
        "    # using the given optimizer and loss_fn.\n",
        "    model.train()\n",
        "    loss = 0\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## Note:\n",
        "    ## 1. Zero grad the optimizer\n",
        "    ## 2. Feed the data into the model\n",
        "    ## 3. Slice the model output and label by train_idx\n",
        "    ## 4. Feed the sliced output and label to loss_fn\n",
        "    ## (~4 lines of code)\n",
        "    optimizer.zero_grad()\n",
        "    out = model(data.x, data.adj_t)\n",
        "    loss = loss_fn(out[train_idx], data.y[train_idx].squeeze())\n",
        "    #########################################\n",
        "\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "aJdlrJQhPBsK"
      },
      "outputs": [],
      "source": [
        "# Test function here\n",
        "@torch.no_grad()\n",
        "def test(model, data, split_idx, evaluator, save_model_results=False):\n",
        "    # TODO: Implement a function that tests the model by \n",
        "    # using the given split_idx and evaluator.\n",
        "    model.eval()\n",
        "\n",
        "    # The output of model on all data\n",
        "    out = None\n",
        "\n",
        "    ############# Your code here ############\n",
        "    ## (~1 line of code)\n",
        "    ## Note:\n",
        "    ## 1. No index slicing here\n",
        "    out = model(data.x, data.adj_t)\n",
        "    #########################################\n",
        "\n",
        "    y_pred = out.argmax(dim=-1, keepdim=True)\n",
        "\n",
        "    train_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['train']],\n",
        "        'y_pred': y_pred[split_idx['train']],\n",
        "    })['acc']\n",
        "    valid_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['valid']],\n",
        "        'y_pred': y_pred[split_idx['valid']],\n",
        "    })['acc']\n",
        "    test_acc = evaluator.eval({\n",
        "        'y_true': data.y[split_idx['test']],\n",
        "        'y_pred': y_pred[split_idx['test']],\n",
        "    })['acc']\n",
        "\n",
        "    if save_model_results:\n",
        "        print (\"Saving Model Predictions\")\n",
        "\n",
        "        data = {}\n",
        "        data['y_pred'] = y_pred.view(-1).cpu().detach().numpy()\n",
        "\n",
        "        df = pd.DataFrame(data=data)\n",
        "        # Save locally as csv\n",
        "        df.to_csv('ogbn-arxiv_node.csv', sep=',', index=False)\n",
        "\n",
        "\n",
        "    return train_acc, valid_acc, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "o7F46xkuLiOL"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'device': 'cuda',\n",
              " 'dropout': 0.5,\n",
              " 'epochs': 100,\n",
              " 'hidden_dim': 256,\n",
              " 'lr': 0.01,\n",
              " 'num_layers': 3}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Please do not change the args\n",
        "args = {\n",
        "    'device': device,\n",
        "    'num_layers': 3,\n",
        "    'hidden_dim': 256,\n",
        "    'dropout': 0.5,\n",
        "    'lr': 0.01,\n",
        "    'epochs': 100,\n",
        "}\n",
        "args"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "dT8RyM2cPGxM"
      },
      "outputs": [],
      "source": [
        "model = GCN(data.num_features, args['hidden_dim'],\n",
        "            dataset.num_classes, args['num_layers'],\n",
        "            args['dropout']).to(device)\n",
        "evaluator = Evaluator(name='ogbn-arxiv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "qd5O5cnPPdVF"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 01, Loss: 4.1829, Train: 21.84%, Valid: 28.11% Test: 25.97%\n",
            "Epoch: 02, Loss: 2.3683, Train: 21.35%, Valid: 21.89% Test: 27.01%\n",
            "Epoch: 03, Loss: 1.9951, Train: 32.70%, Valid: 30.74% Test: 35.12%\n",
            "Epoch: 04, Loss: 1.7985, Train: 32.25%, Valid: 27.65% Test: 25.97%\n",
            "Epoch: 05, Loss: 1.6843, Train: 34.28%, Valid: 26.11% Test: 24.12%\n",
            "Epoch: 06, Loss: 1.5948, Train: 34.96%, Valid: 23.95% Test: 22.10%\n",
            "Epoch: 07, Loss: 1.5330, Train: 36.61%, Valid: 26.64% Test: 26.50%\n",
            "Epoch: 08, Loss: 1.4820, Train: 38.71%, Valid: 32.54% Test: 36.03%\n",
            "Epoch: 09, Loss: 1.4325, Train: 41.01%, Valid: 38.78% Test: 42.85%\n",
            "Epoch: 10, Loss: 1.3898, Train: 43.14%, Valid: 42.30% Test: 45.90%\n",
            "Epoch: 11, Loss: 1.3540, Train: 45.29%, Valid: 45.32% Test: 48.27%\n",
            "Epoch: 12, Loss: 1.3256, Train: 46.97%, Valid: 46.52% Test: 49.53%\n",
            "Epoch: 13, Loss: 1.3046, Train: 48.19%, Valid: 47.60% Test: 50.73%\n",
            "Epoch: 14, Loss: 1.2795, Train: 49.68%, Valid: 49.92% Test: 52.82%\n",
            "Epoch: 15, Loss: 1.2539, Train: 51.35%, Valid: 52.79% Test: 55.33%\n",
            "Epoch: 16, Loss: 1.2360, Train: 52.85%, Valid: 55.16% Test: 57.03%\n",
            "Epoch: 17, Loss: 1.2228, Train: 53.80%, Valid: 55.99% Test: 57.23%\n",
            "Epoch: 18, Loss: 1.2026, Train: 54.56%, Valid: 56.26% Test: 57.28%\n",
            "Epoch: 19, Loss: 1.1891, Train: 55.31%, Valid: 56.43% Test: 57.84%\n",
            "Epoch: 20, Loss: 1.1809, Train: 55.86%, Valid: 56.42% Test: 58.05%\n",
            "Epoch: 21, Loss: 1.1685, Train: 56.40%, Valid: 56.74% Test: 58.43%\n",
            "Epoch: 22, Loss: 1.1562, Train: 57.18%, Valid: 57.49% Test: 59.38%\n",
            "Epoch: 23, Loss: 1.1456, Train: 58.62%, Valid: 59.29% Test: 61.09%\n",
            "Epoch: 24, Loss: 1.1378, Train: 60.08%, Valid: 60.77% Test: 62.39%\n",
            "Epoch: 25, Loss: 1.1329, Train: 60.87%, Valid: 61.69% Test: 63.21%\n",
            "Epoch: 26, Loss: 1.1235, Train: 61.50%, Valid: 62.31% Test: 63.87%\n",
            "Epoch: 27, Loss: 1.1152, Train: 62.29%, Valid: 63.11% Test: 64.70%\n",
            "Epoch: 28, Loss: 1.1106, Train: 63.39%, Valid: 64.20% Test: 65.44%\n",
            "Epoch: 29, Loss: 1.1009, Train: 64.39%, Valid: 65.07% Test: 66.04%\n",
            "Epoch: 30, Loss: 1.0946, Train: 65.31%, Valid: 65.80% Test: 66.68%\n",
            "Epoch: 31, Loss: 1.0880, Train: 66.20%, Valid: 66.58% Test: 67.26%\n",
            "Epoch: 32, Loss: 1.0816, Train: 66.80%, Valid: 67.05% Test: 67.73%\n",
            "Epoch: 33, Loss: 1.0757, Train: 67.26%, Valid: 67.42% Test: 68.16%\n",
            "Epoch: 34, Loss: 1.0722, Train: 67.62%, Valid: 67.52% Test: 68.33%\n",
            "Epoch: 35, Loss: 1.0625, Train: 67.87%, Valid: 67.67% Test: 68.37%\n",
            "Epoch: 36, Loss: 1.0642, Train: 68.10%, Valid: 67.85% Test: 68.43%\n",
            "Epoch: 37, Loss: 1.0577, Train: 68.23%, Valid: 67.86% Test: 68.53%\n",
            "Epoch: 38, Loss: 1.0527, Train: 68.38%, Valid: 68.09% Test: 68.59%\n",
            "Epoch: 39, Loss: 1.0499, Train: 68.80%, Valid: 68.55% Test: 68.79%\n",
            "Epoch: 40, Loss: 1.0464, Train: 69.25%, Valid: 69.08% Test: 69.14%\n",
            "Epoch: 41, Loss: 1.0430, Train: 69.79%, Valid: 69.38% Test: 69.44%\n",
            "Epoch: 42, Loss: 1.0363, Train: 70.25%, Valid: 69.66% Test: 69.58%\n",
            "Epoch: 43, Loss: 1.0331, Train: 70.49%, Valid: 69.78% Test: 69.44%\n",
            "Epoch: 44, Loss: 1.0318, Train: 70.57%, Valid: 69.79% Test: 69.20%\n",
            "Epoch: 45, Loss: 1.0261, Train: 70.60%, Valid: 69.79% Test: 69.38%\n",
            "Epoch: 46, Loss: 1.0217, Train: 70.69%, Valid: 70.03% Test: 69.57%\n",
            "Epoch: 47, Loss: 1.0203, Train: 70.84%, Valid: 70.14% Test: 69.68%\n",
            "Epoch: 48, Loss: 1.0140, Train: 70.92%, Valid: 70.31% Test: 69.64%\n",
            "Epoch: 49, Loss: 1.0173, Train: 71.10%, Valid: 70.43% Test: 69.69%\n",
            "Epoch: 50, Loss: 1.0104, Train: 71.28%, Valid: 70.67% Test: 69.88%\n",
            "Epoch: 51, Loss: 1.0057, Train: 71.41%, Valid: 70.80% Test: 70.02%\n",
            "Epoch: 52, Loss: 1.0051, Train: 71.58%, Valid: 70.79% Test: 70.01%\n",
            "Epoch: 53, Loss: 1.0016, Train: 71.69%, Valid: 70.63% Test: 69.49%\n",
            "Epoch: 54, Loss: 1.0007, Train: 71.75%, Valid: 70.55% Test: 69.38%\n",
            "Epoch: 55, Loss: 0.9945, Train: 71.79%, Valid: 70.62% Test: 69.33%\n",
            "Epoch: 56, Loss: 0.9946, Train: 71.71%, Valid: 70.78% Test: 69.84%\n",
            "Epoch: 57, Loss: 0.9885, Train: 71.79%, Valid: 70.83% Test: 70.17%\n",
            "Epoch: 58, Loss: 0.9889, Train: 71.94%, Valid: 70.95% Test: 70.10%\n",
            "Epoch: 59, Loss: 0.9869, Train: 72.04%, Valid: 71.03% Test: 70.18%\n",
            "Epoch: 60, Loss: 0.9817, Train: 72.10%, Valid: 71.21% Test: 70.34%\n",
            "Epoch: 61, Loss: 0.9795, Train: 72.09%, Valid: 71.06% Test: 70.44%\n",
            "Epoch: 62, Loss: 0.9808, Train: 71.97%, Valid: 70.91% Test: 70.59%\n",
            "Epoch: 63, Loss: 0.9787, Train: 71.86%, Valid: 70.76% Test: 70.53%\n",
            "Epoch: 64, Loss: 0.9738, Train: 72.07%, Valid: 70.82% Test: 70.51%\n",
            "Epoch: 65, Loss: 0.9704, Train: 72.39%, Valid: 71.06% Test: 70.42%\n",
            "Epoch: 66, Loss: 0.9663, Train: 72.51%, Valid: 71.11% Test: 70.10%\n",
            "Epoch: 67, Loss: 0.9663, Train: 72.49%, Valid: 70.89% Test: 69.92%\n",
            "Epoch: 68, Loss: 0.9651, Train: 72.54%, Valid: 70.92% Test: 69.84%\n",
            "Epoch: 69, Loss: 0.9637, Train: 72.54%, Valid: 70.99% Test: 69.78%\n",
            "Epoch: 70, Loss: 0.9608, Train: 72.58%, Valid: 71.29% Test: 70.30%\n",
            "Epoch: 71, Loss: 0.9592, Train: 72.52%, Valid: 71.08% Test: 70.02%\n",
            "Epoch: 72, Loss: 0.9580, Train: 72.49%, Valid: 71.08% Test: 69.84%\n",
            "Epoch: 73, Loss: 0.9538, Train: 72.80%, Valid: 71.44% Test: 70.50%\n",
            "Epoch: 74, Loss: 0.9516, Train: 72.87%, Valid: 71.39% Test: 70.21%\n",
            "Epoch: 75, Loss: 0.9502, Train: 72.82%, Valid: 70.93% Test: 69.57%\n",
            "Epoch: 76, Loss: 0.9496, Train: 72.82%, Valid: 70.89% Test: 69.48%\n",
            "Epoch: 77, Loss: 0.9483, Train: 72.89%, Valid: 71.23% Test: 70.29%\n",
            "Epoch: 78, Loss: 0.9437, Train: 72.93%, Valid: 71.43% Test: 70.40%\n",
            "Epoch: 79, Loss: 0.9449, Train: 73.01%, Valid: 71.33% Test: 70.41%\n",
            "Epoch: 80, Loss: 0.9433, Train: 73.10%, Valid: 71.41% Test: 70.34%\n",
            "Epoch: 81, Loss: 0.9391, Train: 73.19%, Valid: 71.59% Test: 70.77%\n",
            "Epoch: 82, Loss: 0.9407, Train: 73.08%, Valid: 71.56% Test: 70.55%\n",
            "Epoch: 83, Loss: 0.9369, Train: 73.12%, Valid: 71.15% Test: 69.69%\n",
            "Epoch: 84, Loss: 0.9359, Train: 73.18%, Valid: 71.24% Test: 69.83%\n",
            "Epoch: 85, Loss: 0.9350, Train: 73.20%, Valid: 71.56% Test: 70.67%\n",
            "Epoch: 86, Loss: 0.9325, Train: 73.33%, Valid: 71.36% Test: 70.27%\n",
            "Epoch: 87, Loss: 0.9304, Train: 73.33%, Valid: 70.76% Test: 68.98%\n",
            "Epoch: 88, Loss: 0.9284, Train: 73.31%, Valid: 71.00% Test: 69.20%\n",
            "Epoch: 89, Loss: 0.9266, Train: 73.38%, Valid: 71.62% Test: 70.76%\n",
            "Epoch: 90, Loss: 0.9275, Train: 73.30%, Valid: 71.63% Test: 70.94%\n",
            "Epoch: 91, Loss: 0.9245, Train: 73.35%, Valid: 71.27% Test: 70.10%\n",
            "Epoch: 92, Loss: 0.9221, Train: 73.47%, Valid: 71.08% Test: 69.51%\n",
            "Epoch: 93, Loss: 0.9235, Train: 73.62%, Valid: 71.70% Test: 70.79%\n",
            "Epoch: 94, Loss: 0.9235, Train: 73.53%, Valid: 71.74% Test: 70.80%\n",
            "Epoch: 95, Loss: 0.9195, Train: 73.65%, Valid: 71.57% Test: 70.35%\n",
            "Epoch: 96, Loss: 0.9142, Train: 73.72%, Valid: 71.94% Test: 71.01%\n",
            "Epoch: 97, Loss: 0.9171, Train: 73.30%, Valid: 71.53% Test: 71.44%\n",
            "Epoch: 98, Loss: 0.9148, Train: 73.54%, Valid: 71.79% Test: 71.22%\n",
            "Epoch: 99, Loss: 0.9111, Train: 73.44%, Valid: 70.75% Test: 68.98%\n",
            "Epoch: 100, Loss: 0.9157, Train: 73.75%, Valid: 71.33% Test: 69.93%\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "\n",
        "# reset the parameters to initial random value\n",
        "model.reset_parameters()\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=args['lr'])\n",
        "loss_fn = F.nll_loss\n",
        "\n",
        "best_model = None\n",
        "best_valid_acc = 0\n",
        "\n",
        "for epoch in range(1, 1 + args[\"epochs\"]):\n",
        "    loss = train(model, data, train_idx, optimizer, loss_fn)\n",
        "    result = test(model, data, split_idx, evaluator)\n",
        "    train_acc, valid_acc, test_acc = result\n",
        "    if valid_acc > best_valid_acc:\n",
        "        best_valid_acc = valid_acc\n",
        "        best_model = copy.deepcopy(model)\n",
        "    print(f'Epoch: {epoch:02d}, '\n",
        "          f'Loss: {loss:.4f}, '\n",
        "          f'Train: {100 * train_acc:.2f}%, '\n",
        "          f'Valid: {100 * valid_acc:.2f}% '\n",
        "          f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQtt-EKA8P4r"
      },
      "source": [
        "### Question: What are your `best_model` validation and test accuracies?\n",
        "\n",
        "Run the cell below to see the results of your best of model and save your model's predictions to a file named *ogbn-arxiv_node.csv*. \n",
        "\n",
        "You can view this file by clicking on the *output* folder on the right side pannel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "EqcextqOL2FX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving Model Predictions\n",
            "Best model: Train: 73.72%, Valid: 71.94% Test: 71.01%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:81: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        }
      ],
      "source": [
        "best_result = test(best_model, data, split_idx, evaluator, save_model_results=True)\n",
        "train_acc, valid_acc, test_acc = best_result\n",
        "print(f'Best model: '\n",
        "      f'Train: {100 * train_acc:.2f}%, '\n",
        "      f'Valid: {100 * valid_acc:.2f}% '\n",
        "      f'Test: {100 * test_acc:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7JXsMTBgeOI"
      },
      "source": [
        "Once you have working code for each cell above, **save a version in Kaggle and share your notebook**!"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
